**Paper: ** Gustavo Publio, Diego Esteves, Agnieszka Ławrynowicz, Panče Panov, Larisa Soldatova, Tommaso Soru and Joaquin Vanschoren: ML Schema: Exposing the Semantics of Machine Learning with Schemas and Ontologies ([preprint](submission/publio/index.html), [postprint](accepted/publio/index.html))

**Decision: reject** 

**Review 1 (anonymous)** 
This is a good read & good work: a well written paper based on authors' activities within the according W3C community group. The story of the paper is straightforward and makes the point: there are existing vocabularies & ontologies, and now ML Schema provides not only more elaborated model (in terms of coverage of different ML aspects) but also serves as a glue between existing work.

Authors do not make a too strong connection to SAVE-SD topics but obviously if there is a way to better semantify ML and its parts & different settings this will also greatly support scholarly communication.

All in all, I think this paper will fit nicely the workshop program and should spark interesting discussions, hence an "accept".

Just one comment (or merely a set of questions). Authors write "The vocabulary and axiomatization included in those resources may be used to make explicit the semantics of ML models, making them better interpretable for human users." but now I am wondering how about serving various robots as well?

Further on this: if ML models are better interpretable for robotic users, what can that mean? Do you foresee benefits? Can we soon evidence a day when a robot decides quite automatically which ML approach to use for a given task (I know that some attempts to that direction exist) with the help of ML Schema?

Finally, is there somewhere data about ML models & experiments annotated using ML Schema? And if not, why not? I would certainly like to check.

**Review 2 (anonymous)** 
The authors present the ML Schema and argue that exposing semantics of machine learning algorithms, datasets, and experiments to achieve the full interoperability of experiments.

Figure 1 is confusing because it contains many arrows between the components inside without explanation in the text. In addition, there are several acronyms without mentioning the full title such as MLF, IDE, and SWFS.
Paper organization is missing in the introduction.
Section 2, which presents the main contribution of the work, is too short and loses the description of the ML schema. Even the main concept (Run) has been neither discussed nor mentioned in the text. That affects the information density in the paper.
Referring to Fig.2, it is a bit strange to me how “Run” hasOutput “Model” and “MoedlEvaluation”?
Table 1, takes much space without a big contribution, again, information density.
- The most of the text in section 3 has been copied from other resources.
- The conclusion is very short and there is no future work.

Message to authors:
I would like to remind that direct copy of sentences and paragraphs, even if they are yours, is allowed. Please re-write, re-phrase and summarize the whole paper and resubmit, maybe to another venue or SAVE-SD 2019.

**Review 3 (anonymous)** 
This paper introduces a schema meant to describe machine learning models in order to facilitate reuse and interoperability. The intention behind this short paper has its merits, however, I have to admit that I found the paper somewhat poorly presented, significantly lacking in clarity, and the actual schema presentation raised several key questions.

First of all, the paper discusses a "schema to describe machine learning algorithms,...", while, what you really seem after is a representation of "machine learning models" (and their provenance). Then, the paper does not even clarify whether it's main intention is to address supervised or unsupervised learning. It appears that the authors have supervised learning in mind, yet this is nowhere stated.

Figure 1 seems not very well explained (what is "MLF"?, what is "SWFS"? etc ). Figure 2, i.e. the actual schema, also raises several doubts: e.g., why is "Feature" a subclass of "Dataset"? What is "DataCharacteristic" or "DatasetCharacteristic" or "FeatureCharacteristic" precisely? Shouldn't the "ModelEvaluation" be tied to the "Model" and not the "Run"? The evaluation procedure also seems underspecified to me.

Given the provided information, this work seems not still in its early stages, i.e. presenting some work in progress, which deserves more work and clarity w.r.t. presentation.

**Review 4 (anonymous)** 
This paper describes the ML-Schema, a comprehensive schema for modeling machine learning algorithms, datasets, and experiments, which aligns more fine-grained ontologies and vocabularies in the Machine Learning domain.

It may well be that this is a very useful addition to current ontologies for describing computer science efforts, but although the abstract promises us that the paper will ‘argue that exposing [the] semantics of machine learning algorithms, models, and experiments may pave the way to better interpretability’, this is not the case. The paper simply presents the ontology, and that is that.

As such, this is not a scientific paper: it does not state a research question, does not present any related work, it does not explain how the ontology was developed, whether there were any issues or lessons learned in this process; it also does not compare the final outcome to other work, and does not draw any conclusions regarding the usefulness of the work done.

If those issues were added, this could be a useful paper. As it stands, I am afraid, it is less informative than e.g. the page describing the draft ML schema, created by the same group: http://htmlpreview.github.io/?https://github.com/ML-Schema/documentation/blob/gh-pages/ML%20Schema.html, and does not merit acceptance as a conference paper.

If the paper is rewritten the paper with the above comments in mind, I would be happy to rereview it.

**Meta-review (anonymous)** 
This paper is insufficiently elaborated. A new ontology is presented to describe "information on machine learning algorithms, datasets, and experiments". While this is potentially interesting, the paper actually fails to discuss the ontology in sufficient detail. In addition, as pointed out by some reviewers, the information presented in this paper has already been published and has been presented better in other publications. Hence, it is hard to see what is the new contribution provided here. And, aside from this problem, in any case the paper is not developed enough even for a workshop submission.

- - -

**Paper: ** Gabriele Scalia, Matteo Pelucchi, Alessandro Stagni, Tiziano Faravelli and Barbara Pernici: Storing combustion data experiments: new requirements emerging from a first prototype ([preprint](submission/scalia/index.pdf), [postprint](accepted/scalia/index.pdf))

**Decision: accept** 

**Review 1 (anonymous)** 
This paper presents an approach on solving data sharing and integration challenges in the field of combustion research. The authors identify and explain requirements and sketch a possible architecture. The paper is well written, but it is unclear what its type and main message are.

The requirements are described in detail, but their proposed architecture is only described very briefly and informally. The title mentions a prototype, but sections 4 and 5 are silent about any existing partial implementation.

The paper is labeled as a position paper, but it is not clear what position is put forward. It looks more like a requirements analysis for a very specific use case. Many of the observed problems seem to be general and not restricted to the use case, but the nature and objective of the paper is not properly explained. It seems like a mixture of an incomplete Systems Paper and an ill-focused Position Paper.

However, I believe the paper has an interesting message to tell if it is properly framed as a position paper. Therefore, I think it can be accepted but a considerable amount of revision work would have to be done.

Some more minor comments:

- It seems very weird to use exponential notation for small numbers such as 100, 1000, and 10000 (page 3).

- The source of the content of Figure 1 should be mentioned.

- "semantics" instead of "semantic" when used as a noun

**Review 2 (anonymous)** 
The paper presents a very comprehensive overview of the solutions available for storing combustion data experiments in the chemical kinetics domain. It also defines requirements for the scenario in which such solutions can be made interoperable and used together, instead of checking each of them.

The paper provides a really nice introduction to the domain and overview of existing solutions. The description of the requirements is nicely structured and is also clear. It seems to be novel, and constitutes the main contribution of the paper. However, the proposed architecture is hardly making a substantial contribution, because, as far as I understood from the paper, it is just an idea which was not tested. In other similar solutions, e.g., if one wants to build a repository harvesting bibliographic data via OAI-PMH protocol from various preprint repositories, one would need: input files from the external repositories, a kind of ETL process to load the data, a database (triple store) to store the data and some infrastructure on top to support the data management. Complement this with some reporting/analytics functionality and generation of output files in a specific format and, voila, we get the architecture very similar to Fig. 2. It would be great to see that architecture tested in practice, while the current paper can just focus on the requirements.

In the following, I will provide more specific comments on the paper, hoping they can help the authors to improve the paper further:

1. It would be nice to highlight which of the listed requirements are specific to the domain and which exist in other domains, so that (parts of) the solution can be reused?

2. "The urgent need of improving the infrastructure supporting the reuse of scholarly data ...". I would advise adding citation to support this claim and also call the data "experimental", as "scholarly" has several meanings, e.g., in certain contexts it might by synonymous to "bibliometric".

3. The paper several times mention "EU projects" but only give one example (Smartcats.eu)

4. end of Section 2 - for storing the data, supplementary materials and other artifacts related to the paper, there is a number of repositories available. Some of them are discipline-specific, but some are generic (and free!) - like Figshare. I wonder if those solutions could be used here. See https://www.nature.com/sdata/policies/repositories for a possible list of repositories.

5. For reference 8, CAvallotti 2017, it is advisable to have at least a link to the paper/preprint, otherwise a reader cannot access it.

The paper is nicely written and very easy to read even for a non-expert in the combustion domain like myself. I encourage authors to continue this work and to have one of the next papers on "Google Scholar for combustion experiments".

**Review 3 (anonymous)** 
This position paper looks at the area of combustion data experiments, at the current practices and existing repositories, to extract a set of detailed requirements regarding the dicovery, enrichement, validation and general management of such experimental data. The paper also briefly describes a general architecture to address those requirements. The contribution of the paper is not very concrete but the general discussion based on a specific area of research is certainly very valuable for the workshop. Part of the discussion could in particular be on how much of the requirements (and therefore components of the architecture) would be shared with other domains (I suspect a lot).

**Review 4 (anonymous)** 
In the paper the challenges related to the management and use of experimental data in the field of chemical kinetics modelling are discussed. The authors conclude that a successful solution could consist in a service-oriented infrastructure based on semantic data model for the domain.

Initially, the authors provide a good introduction to kinetics modeling by describing the characteristics of combustion experiments, resulting data and its modelling, both undergoing iterative improvements. The authors name a few research groups and initiatives active in the domain, and point to some repositories and tools that deal with such data.
Next, some use cases to take into account while constructing the infrastructure for kinetics data management are given. The authors identify requirements for efficient and effective data management; for some of them possible technical solutions to automate the process are mentioned.
Finally, the authors propose a modular system of independent services that could facilitate data acquisition, exploration, integration and processing. They conclude that individual components could take advantage of semantic data model. As it is mentioned, a complete domain ontology does not currently exist.

It seems that common conceptualisation of the information in the domain could indeed significantly improve interoperability of the existing data for the mentioned use cases and help to manage the evolution of the system. It would be interesting to learn how the system requirements identified in the paper translate to properties required from this data model, e.g., as the number of object's parameters can modified over time based on relationships with other objects acquired by the system, the questions of the object's state and its provenance need to be addressed.

The paper's section on the proposed architecture could be developed a bit more, and the draft of the infrastructure presented in Fig. 2 could be improved to provide more precise links between individual components. Fig.1 is difficult to read due to small font size and seems cut on on side. Overall, an interesting and unfolding topic.

**Meta-review (anonymous)** 
Having looked at the main reviews: All reviewers note the same strengths and weaknesses of the paper, only the evaluation note is different. I agree with the overall review statements. To bottom line:

Pro: the requirements of the data environment, though may be a bit specific to the application area, are of interest and a source of valuable discussion during the Workshop.

Con: the implementation draft is sketchy at best. I would add to the various reviewer notes that the terms "knowledge graph", RDF, "graph database", "semantic graph" are all similar notion but only that: similar. They do not cover identical techniques and tools, and a conversion between, say, RDF and a property graph structure underpinning, e.g., neo4j is an added complexity by itself. (Alas!). It is absolutely unclear from the paper whether the authors realize this.